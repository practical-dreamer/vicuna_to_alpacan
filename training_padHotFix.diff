--- original.py	2023-04-30 02:21:11.970945304 -0500
+++ modified.py	2023-04-30 02:28:53.225878103 -0500
@@ -243,7 +243,7 @@
         return
 
     gradient_accumulation_steps = batch_size // micro_batch_size
-    shared.tokenizer.pad_token = 0
+    shared.tokenizer.pad_token_id = 0
     shared.tokenizer.padding_side = "left"
 
     def tokenize(prompt):
