--- original.py	2023-04-29 22:55:39.914537093 -0500
+++ modified.py	2023-04-29 22:41:18.716386000 -0500
@@ -411,8 +411,33 @@
     if WANT_INTERRUPT:
         yield "Interrupted before start."
         return
+        
+    def log_train_dataset(trainer):
+        from datetime import datetime
+
+        # Get current date and time
+        now = datetime.now()
+        date_time = now.strftime("%Y%m%d_%H%M%S")
+
+        # Create a log filename with the current date and time
+        log_filename = f"train_dataset_log_{date_time}.txt"
+
+        # Create or open a log file in the logs directory
+        with open(f'logs/{log_filename}', 'a') as log_file:
+            # Iterate over the entire dataset
+            for i in range(len(trainer.train_dataset)):
+                # Decode the 'input_ids' from each element in the dataset
+                decoded_text = shared.tokenizer.decode(trainer.train_dataset[i]['input_ids'],skip_special_tokens=False)
+                # Append the decoded text to the log file
+                log_file.write(decoded_text)
+                # Add separator line
+                log_file.write("\n---------------ðŸ¦™TRAINER LOG DIVIDERðŸ˜‰---------------\n")
+
+        # Print log file creation confirmation
+        print(f"Log file '{log_filename}' created in the 'logs' directory. ðŸ¦™ðŸ˜‰")
 
     def threaded_run():
+        log_train_dataset(trainer)
         trainer.train()
         # Note: save in the thread in case the gradio thread breaks (eg browser closed)
         lora_model.save_pretrained(lora_file_path)
